# Copyright 2021 Ramon Casero 
# SPDX-License-Identifier: Apache-2.0
# -*- coding: utf-8 -*-
"""Neural network architectures and related functions.

Functions:
    train_loop: Run one epoch of training in neural network.
    test_loop: Run one epoch of testing/validating in neural network.
    SimpleLinearNN: Simple PyTorch neural network with 3 fully connected layers.
    SimpleCNN: Simple PyTorch convolutional neural network with variable number of convolutional blocks.
"""

import numpy as np
import torch
import sklearn.metrics


def train_loop(args, dataloader, model, loss_fn, optimizer):
    """Run one epoch of training in neural network.

    :param args: (Namespace) Object generated by an input arguments parser (ConfigArgParse or ArgParse).
    :param dataloader: (torch.utils.data.DataLoader) Dataloader for training dataset.
    :param model: (torch.nn.Module) PyTorch neural network.
    :param loss_fn: (torch.nn.modules.Loss) PyTorch loss function.
    :param optimizer: (torch.optim.Optimizer) PyTorch optimizer function.
    :return:
    * metrics: (dict)
      - 'mean_loss': (float) Mean loss per image.
    """

    data_size = len(dataloader.sampler)
    num_batches = len(dataloader)
    mean_loss = 0.0

    for batch, (x, y) in enumerate(dataloader):
        # if necessary, move batch to GPU memory (or whatever device we are using)
        x, y = x.to(args.device), y.to(args.device)

        # compute prediction and loss
        model.train()
        score = model(x)
        loss = loss_fn(score, y)
        mean_loss += loss.item()

        # backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if args.verbose and batch % 100 == 0:
            loss, current = loss.item(), batch * len(x)
            print(f'train mean loss: {loss:>.4f}  [{current:>6d}/{data_size:>6d}]')

    # this translates to mean loss per image
    mean_loss /= num_batches

    metrics = {'mean_loss': mean_loss}
    return metrics


def test_loop(args, dataloader, model, loss_fn, class_idx=None):
    """Run one epoch of testing/validating in neural network.

    :param args: (Namespace) Object generated by an input arguments parser (ConfigArgParse or ArgParse).
    :param dataloader: (torch.utils.data.DataLoader) Dataloader for testing/validating dataset.
    :param model: (torch.nn.Module) PyTorch neural network.
    :param loss_fn: (torch.nn.modules.Loss) PyTorch loss function.
    :param class_idx: (list of ints, def None) List of classifier indices. The confusion matrix will be generated only
    for these indices. If class_idx=None, class_idx will be computed as all the indices found in the dataloader labels.
    :return:
    * metrics: (dict)
      - 'mean_loss': (float) Mean loss per image.
      - 'accuracy': (float) Accuracy of network on whole dataset.
      - 'cm': (np.ndarray) Confusion matrix for the labels of the dataset, normalised so that the sum of each row = 1.
      In cm, the rows are true label values, and the columns are network predictions.
    """

    size = len(dataloader.sampler)
    num_batches = len(dataloader)
    mean_loss = accuracy = 0.0
    cm = 0

    if len(dataloader) == 0:
        cm = sklearn.metrics.confusion_matrix([], [], labels=class_idx)
        metrics = {'mean_loss': np.nan, 'accuracy': np.nan, 'cm': cm}
        return metrics

    with torch.no_grad():

        # batch loop
        for batch, (x, y) in enumerate(dataloader):
            # if necessary, move batch to GPU memory (or whatever device we are using)
            x, y = x.to(args.device), y.to(args.device)

            # softmax scores and label prediction according to the largest score
            model.eval()
            score = model(x)
            pred = score.argmax(1)

            # add loss and accuracy of this batch to the metrics for the whole dataset
            mean_loss += loss_fn(score, y).item()
            accuracy += (pred == y).type(torch.float).sum().item()

            cm += sklearn.metrics.confusion_matrix(y.cpu(), pred.cpu(), labels=class_idx)

    # normalize confusion matrix over the predictions (sum of each row = 100%)
    cm = 100 * cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    # average loss and accuracy
    mean_loss /= num_batches
    accuracy /= size

    metrics = {'mean_loss': mean_loss, 'accuracy': accuracy, 'cm': cm}
    return metrics


class SimpleLinearNN(torch.nn.Module):
    """Simple PyTorch neural network with 3 fully connected layers.

    This is a small network that can be used for quick tests. The network layout for input_size=(28, 28), class_num=10
    is

    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    SimpleLinearNN                           --                        --
    ├─Flatten: 1-1                           [1, 784]                  --
    ├─Sequential: 1-2                        [1, 10]                   --
    │    └─Linear: 2-1                       [1, 512]                  401,920
    │    └─ReLU: 2-2                         [1, 512]                  --
    │    └─Linear: 2-3                       [1, 512]                  262,656
    │    └─ReLU: 2-4                         [1, 512]                  --
    │    └─Linear: 2-5                       [1, 10]                   5,130
    ==========================================================================================
    """

    def __init__(self, input_size=(28, 28), class_num=10):
        """SimpleLinearNN initialization.

        :param input_size: ((H, W), def (28, 28)). Size of input images.
        :param class_num: (int, def 10). Number of output features, i.e. classes or labels.
        """
        super(SimpleLinearNN, self).__init__()
        self.flatten = torch.nn.Flatten()
        self.linear_relu_stack = torch.nn.Sequential(
            torch.nn.Linear(input_size[0] * input_size[1], 512),
            torch.nn.ReLU(),
            torch.nn.Linear(512, 512),
            torch.nn.ReLU(),
            torch.nn.Linear(512, class_num),
        )

    def forward(self, x):
        """SimpleLinearNN forward pass.

        :param x: (torch.Tensor) Input images to the network in (batch, 1, H, W) tensor form.
        :return:
        * logits: (np.ndarray) Vector with the logits (network predictions before softmax) that correspond to the input
        images.
        """
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits


# SimpleCNN neural network
class SimpleCNN(torch.nn.Module):
    """Simple PyTorch convolutional neural network with variable number of convolutional blocks.

    The convolutional blocks have the layers:
        Conv2d
        ReLU
        MaxPool2d
        BatchNorm2d

    The network output contains a 1x1 convolution and fully-connected layer formed by:
        Conv2d
        Flatten
        Linear
    """

    def __init__(self, input_size=(28, 28), conv_out_features=[8, 16], conv_kernel_size=3, maxpool_kernel_size=2,
                 class_num=10):
        """SimpleCNN initialization.

        Example: input_size=(28, 28), conv_out_features=[8, 16], conv_kernel_size=3, maxpool_kernel_size=2
        produces the network

        ==========================================================================================
        Layer (type:depth-idx)                   Output Shape              Param #
        ==========================================================================================
        SimpleCNN                                --                        --
        ├─ModuleList: 1-1                        --                        --
        │    └─Conv2d: 2-1                       [1, 8, 28, 28]            80
        │    └─ReLU: 2-2                         [1, 8, 28, 28]            --
        │    └─MaxPool2d: 2-3                    [1, 8, 14, 14]            --
        │    └─BatchNorm2d: 2-4                  [1, 8, 14, 14]            16
        │    └─Conv2d: 2-5                       [1, 16, 14, 14]           1,168
        │    └─ReLU: 2-6                         [1, 16, 14, 14]           --
        │    └─MaxPool2d: 2-7                    [1, 16, 7, 7]             --
        │    └─BatchNorm2d: 2-8                  [1, 16, 7, 7]             32
        │    └─Conv2d: 2-9                       [1, 1, 7, 7]              145
        │    └─Flatten: 2-10                     [1, 49]                   --
        │    └─Linear: 2-11                      [1, 10]                   500
        ==========================================================================================

        :param input_size: ((H, W), def (28, 28)) Size of input images.
        :param conv_out_features: (list of ints, def [8, 16]) Each element in the list creates a new convolutional
        block with the corresponding number of output features.
        :param conv_kernel_size: (int, def 3) Size of convolutional kernels in convolutional blocks.
        :param maxpool_kernel_size: (int, def 2) Size of MaxPool kernels in convolutional blocks.
        :param class_num: (int, def 10) Number of output classes or labels in the network.
        """
        super(SimpleCNN, self).__init__()

        # dummy input to compute tensor shape as it progresses down the network
        x = torch.ones((1, 1) + tuple(input_size))

        self.cnn_stack = []

        # loop for convolutional blocks
        for out_channels in conv_out_features:
            self.cnn_stack.append(torch.nn.Conv2d(in_channels=x.shape[1], out_channels=out_channels,
                                                  kernel_size=conv_kernel_size, stride=1, padding=1))
            x = self.cnn_stack[-1](x)
            self.cnn_stack.append(torch.nn.ReLU())
            x = self.cnn_stack[-1](x)
            self.cnn_stack.append(torch.nn.MaxPool2d(kernel_size=maxpool_kernel_size))
            x = self.cnn_stack[-1](x)
            self.cnn_stack.append(torch.nn.BatchNorm2d(num_features=out_channels))
            x = self.cnn_stack[-1](x)

        # 1x1 convolution + fully connected output block
        self.cnn_stack.append(torch.nn.Conv2d(in_channels=x.shape[1], out_channels=1,
                                              kernel_size=conv_kernel_size, stride=1, padding=1))
        x = self.cnn_stack[-1](x)
        self.cnn_stack.append(torch.nn.Flatten())
        x = self.cnn_stack[-1](x)
        self.cnn_stack.append(torch.nn.Linear(in_features=x.shape[1], out_features=class_num))

        self.cnn_stack = torch.nn.ModuleList(self.cnn_stack)

    def forward(self, x):
        """SimpleCNN forward pass.

        :param x: (torch.Tensor) Input images to the network in (batch, 1, H, W) tensor form.
        :return:
        * logits: (np.ndarray) Vector with the logits (network predictions before softmax) that correspond to the input
        images.
        """
        for module in self.cnn_stack:
            x = module(x)
        return x  # logits
